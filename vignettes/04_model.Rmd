---
title: "Build predictive model"
author: "Fabien Jossaud, Florent Chuffart"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
---



```{r echo=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment = "#>", fig.width=9, fig.height=6, eval=TRUE, echo=FALSE, results="hide")
start_time = Sys.time()
source("common.R")
```

```{r params_default, echo=FALSE}
source("params_default.R")
if (!exists("seed")) seed = 1
  if (!exists("scenario")) scenario = "A"
```

```{r building_indexes, echo=FALSE}
print("Loading data")
df = mreadRDS(paste0("df_preproc_",gse,".rds"))

# idx_samples = rownames(df)
# markers_start = grep("cg",colnames(df))[1]
# idx_clinicals = colnames(df)[1:(markers_start-1)]
# idx_cpg = colnames(df)[markers_start:ncol(df)]
# gender01 = as.numeric(df$gender)
# library(EpiDISH)
# hannumData.m = t(mget_full_cpg_matrix(gse))[,idx_samples]
# BloodFrac.m <- epidish(hannumData.m, ref.m = centDHSbloodDMC.m, method = "RPC")$estF
# head(BloodFrac.m)
# df = cbind(df[,idx_clinicals], gender01, BloodFrac.m, df[,idx_cpg])
# saveRDS(df, paste0("df_preproc_",gse,".rds"))

idx_samples = rownames(df)
markers_start = grep("cg",colnames(df))[1]
idx_clinicals = colnames(df)[1:(markers_start-1)]
idx_cpg = colnames(df)[markers_start:ncol(df)]
```

## Model

```{r}
if (scenario=="A") {
  confounders = NULL  
} else if (scenario=="B") {
  confounders = "gender01"
} else if (scenario=="C") {
  confounders = c(
    # "B"      ,
    "NK"     ,
    "CD4T"   ,
    "CD8T"   ,
    "Gran"   ,
    "Mono"   ,
    "Neutro" ,
    "Eosino"
  )
} else if (scenario=="D") {
  confounders = c(
    "gender01"      ,
    "NK"     ,
    "CD4T"   ,
    "CD8T"   ,
    "Gran"   ,
    "Mono"   ,
    "Neutro" ,
    "Eosino"
  )
} else {
  confounders = NULL  
}
```

`r paste0("~", paste0(c(y_key, confounders), collapse="+"))`


## Training and test sets 

```{r parameters, echo=TRUE, results="verbatim"}
print(paste0("Use ", nb_train, " observations for training."))
if (!exists("MODESA")) {
  set.seed(seed)
  idx_train = sample(rownames(df), nb_train)
  idx_test = setdiff(rownames(df), idx_train)  
} else { # Sensitivity analysis
  idx_test = rownames(mreadRDS(paste0("df_",gse,".rds")))[1:100]
  idx_test = idx_test[idx_test %in% rownames(df)] 
  idx_train = setdiff(rownames(df), idx_test)   
  nb_train = length(idx_train)
}
```

## Filtering probes using EWAS 


```{r ewas}
if (nbewasprobes>0) {
  print("filtering EWAS top probes")
  x = mget_full_cpg_matrix(gse, idx_train, idx_cpg)
  cpg_matrix = t(x)
  if (!is.null(confounders)) {
    design_matrix <- model.matrix(formula(paste0("~", y_key, "+", paste0(confounders, collapse="+"))), df[idx_train,idx_clinicals])    
  } else {
    design_matrix <- model.matrix(formula(paste0("~", y_key)), df[idx_train,idx_clinicals])    
  }
  dim(design_matrix)
  dim(cpg_matrix)
  fit = mlimma_lmFit(cpg_matrix, design_matrix)
  fit = limma::eBayes(fit) # compute moderated t-statistics, moderated F-statistic, and log-odds of differential expression by empirical Bayes moderation of the standard errors towards a global value
  # head(fit$p.value)
  idx_cpg_oi = rownames(fit$p.value)[rank(fit$p.value[,2]) <= nbewasprobes]
  layout(1, respect=TRUE)
  plot(fit$coefficients[,2], -log10(fit$p.value[,2]), col=(rownames(fit$p.value) %in% idx_cpg_oi)+1, pch=".", main=paste0(nbewasprobes, " probes"))

  litterature_models = readRDS("litterature_models.rds")  
  idx_cpg_litt = unique(unlist(lapply(lapply(litterature_models, "[[", "coeff"), "[[", "probes")))  

  idx_cpg = intersect(idx_cpg, c(idx_cpg_oi, idx_cpg_litt))

  dim(df)
  df = df[,c(idx_clinicals, idx_cpg)]
  dim(df)


  # df_dnamaging = df
  # colnames(df_dnamaging)[2] = "age"
  # head(df_dnamaging[,1:20])
  # save(df_dnamaging, file='~/projects/dnamaging/data/df_dnamaging.RData' , compress='xz')
} else {
  # Nothing to do for idx_cpg
}








```


## Elastic net regularization

At the first time, we use elastic net method to build our first model. To use this method, we need 2 parameters $\alpha$ (lasso and ridge regression rate) and $\lambda$ (penalization parameter for penalized regression). We decide to use cross validation to first find the best $\lambda$ for each $\alpha$ we have. On the left you have the mean of cross validation RMSE depending on $\lambda$ and $\alpha$ and on the right you have the mean of cross validation probes taken for the model depending on $\alpha$ and $\lambda$. Here, the different $\alpha$ are chosen with the function cva.glmnet. As we can see, the RMSE really differ depending on what $\lambda$ we take for each $\alpha$. But we also see that RMSE from the best $\lambda$ for each $\alpha$ doesn’t differ much depending on $\alpha$. Every RMSE from best $\lambda$ for each $\alpha$ are in the RMSE interval (mean $\pm$ sd) of the best $\alpha$,$\lambda$ couple (here 0.216,1.72). So we verify after in the right plot the number of probes for our best couple to see if we have a good number of probes to avoid overfitting. So we can select our best $\alpha$,$\lambda$ couple to build our model with an elastic net regression. 
For information, Elastic net regularization is a method consisting in mix Lasso and Ridge regression methods. We have the metaparameter lambda for Ridge and Lasso regression model (regularization parameter) and the meta parameter alpha corresponding in the proportion of Lasso and Ridge (principe of Elastic net regression). Cross validation is a process consisting in cutting the train set in k folds. We learn our model on k-1 folds and validate it on the last fold, and we make that k times to validate the model on each folds. We can after make the mean mse to take the best parameters.




```{r cva.glmnet call}
print("Building modelcva...")
modelcva = mcvaglmnet(idx_train=idx_train, y_key=y_key, gse=gse, idx_cpg=idx_cpg, confounders=confounders, alpha=0.2) 

cvarecaps = data.frame(
  lambda   = unlist(lapply(modelcva$modlist, "[[", "lambda")), 
  cvm      = unlist(lapply(modelcva$modlist, "[[", "cvm")   ), 
  cvsd     = unlist(lapply(modelcva$modlist, "[[", "cvsd")   ), 
  cvup     = unlist(lapply(modelcva$modlist, "[[", "cvup")  ),
  cvlo     = unlist(lapply(modelcva$modlist, "[[", "cvlo")  ),
  nbprobes = unlist(lapply(modelcva$modlist, "[[", "nzero") ),
  lambdamin= rep(unlist(lapply(modelcva$modlist, "[[", "lambda.min")) , sapply(lapply(modelcva$modlist, "[[", "lambda"), length)), 
  lambda1se= rep(unlist(lapply(modelcva$modlist, "[[", "lambda.1se")) , sapply(lapply(modelcva$modlist, "[[", "lambda"), length)), 
  alpha    = rep(modelcva$alpha                                       , sapply(lapply(modelcva$modlist, "[[", "lambda"), length)),
  nfolds   = modelcva$nfolds
)
cvarecaps$rmse = sqrt(cvarecaps$cvm)
cvarecaps$rmseup = sqrt(cvarecaps$cvup)
cvarecaps$rmselo = sqrt(cvarecaps$cvlo)
cvrecaps = cvarecaps  

head(cvrecaps)
dim(cvrecaps)
```

```{r best_model_cvaglmnet_params}
tmp_alphas = sort(unique(cvrecaps$alpha))
# tmp_alphas = unlist(sapply(tmp_alphas, function(alpha) {
#   cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
#   if (cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$nbprobes<=1*length(idx_train)) {
#     alpha
#   } else {
#     NULL
#   }
# }))

if (!is.null(tmp_alphas)) {
  sub_cvrecaps = cvrecaps[cvrecaps$alpha%in%tmp_alphas,] 
} else {
  sub_cvrecaps = cvrecaps
}

sub_cvrecaps = cvrecaps[cvrecaps$lambda==cvrecaps$lambda1se,] 
sub_cvrecaps[sub_cvrecaps$cvm==min(sub_cvrecaps$cvm),]


best_model_cvaglmnet_params = sub_cvrecaps[sub_cvrecaps$cvm==min(sub_cvrecaps$cvm),][1,]
best_model_cvaglmnet = mmodel_factory_glmnet(idx_train=idx_train, y_key=y_key, alpha=best_model_cvaglmnet_params$alpha, lambda=best_model_cvaglmnet_params$lambda, gse=gse, idx_cpg=idx_cpg, confounders=confounders) 
best_model_cvaglmnet_probes = as.character(best_model_cvaglmnet$coeff$probes)
rmse = sub_cvrecaps[sub_cvrecaps$cvm==min(sub_cvrecaps$cvm),][1,]$rmse
```


```{r plot cva.glmnet results}
main = paste0("Cross-Validation nb_train=", nb_train)
# Plotting rmse and nbprobes depending on alpha and lambda (with cv results)
layout(matrix(1:2,1),respect = TRUE)
plot(0, 0, col=0, xlab="log10(lambda)", ylab="RMSE", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, min(cvrecaps$rmse)*2))
tmp_alphas = sort(unique(cvrecaps$alpha))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  # cvrecap = sub_cvrecaps[sub_cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$rmse             , col=tmp_col, lty=2)
  points(  log10(sub_recap$lambda), sub_recap$rmse  , col=tmp_col, pch=1)
  cvrecap = sub_cvrecaps[sub_cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  if (best_model_cvaglmnet_params$alpha==alpha & best_model_cvaglmnet_params$lambda==sub_recap$lambda) {
    # stop("EFN")
    points(  log10(sub_recap$lambda), sub_recap$rmse  , col=tmp_col, pch=16)
    arrows(x0=log10(sub_recap$lambda), y0=sub_recap$rmselo, x1=log10(sub_recap$lambda), y1=sub_recap$rmseup, code=3, angle=90, length = 0.03, col=tmp_col, lwd=2)
    abline(h=rmse, v=log10(sub_recap$lambda), col=tmp_col)
    legend("bottomright", paste0("RMSE=", signif(rmse, 3), ", alpha=", signif(alpha, 3), ", lambda=", signif(sub_recap$lambda, 3)), lty=1, cex=.5)
  }    
})


plot(0, 0, col=0, xlab="log10(lambda)", ylab="nbprobes", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, max(nb_train, best_model_cvaglmnet_params$nbprobes*1.5)))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$nbprobes         , col=tmp_col, lty=2)
  points(  log10(sub_recap$lambda), sub_recap$nbprobes  , col=tmp_col, pch=1)
  cvrecap = sub_cvrecaps[sub_cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  if (best_model_cvaglmnet_params$alpha==alpha & best_model_cvaglmnet_params$lambda==sub_recap$lambda) {
    points(  log10(sub_recap$lambda), sub_recap$nbprobes  , col=tmp_col, pch=16)
    abline(h=sub_recap$nbprobes, v=log10(sub_recap$lambda), col=tmp_col)
    legend("bottomright", paste0(sub_recap$nbprobes, " probes"), cex=.5)
  }
})
legend(x="topright", legend=tmp_alphas, fill=1:length(tmp_alphas), title="alpha", cex=.5)
```


```{r echo=TRUE, results="verbatim"}
best_model_cvaglmnet_params
```















```{r loading train/test sets, eval=FALSE}
# #Create Train and Test samples
# Xtrain = mget_full_cpg_matrix(gse, idx_train, idx_cpg)
# Ytrain = mget_df_preproc(gse)[idx_train,y_key]
# Xtest = mget_full_cpg_matrix(gse, idx_test, idx_cpg)
# Ytest = mget_df_preproc(gse)[idx_test,y_key]
# ```
#
# ```{r plot prediction for best model, echo=FALSE, eval=FALSE}
# m = modelcva$modlist[[which(modelcva$alpha==best_model_cvaglmnet_params$alpha)]]
# predTr = predict(m, Xtrain, type="response", s="lambda.min")
# rmseTr = sqrt(mean((Ytrain - predTr)^2))
# predTe = predict(m, Xtest, type="response", s="lambda.min")
# rmseTe = sqrt(mean((Ytest - predTe)^2))
#
# layout(matrix(1:2,1), respect=TRUE)
# plot(Ytrain, predTr, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
# plot(Ytest , predTe, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
# ```
#
# ```{r eval=FALSE}
# m = best_model_cvaglmnet
# # build model
# predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTr = sqrt(mean((Ytrain - predTr)^2))
# predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTe = sqrt(mean((Ytest - predTe)^2))
#
# layout(matrix(1:2,1), respect=TRUE)
# plot(Ytrain, predTr, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
# plot(Ytest , predTe, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
#
#
# build_model_from_probes = function(probes, sub_df, y_key, name) {
#   tmp_m = lm(formula(paste0(y_key,"~0+",paste0(probes,collapse="+"))), data=sub_df) #lm model with probes previously find with bootstrap
#   idx = probes
#   idx = idx[!is.na(tmp_m$coefficients[idx])]
#   beta = tmp_m$coefficients[idx]
#   coeff = data.frame(probes=idx, beta=beta)
#   rownames(coeff) = idx
#   coeff$mean = apply(sub_df[,rownames(coeff)], 2, mean)
#   head(coeff)
#   model = list(coeff=coeff, Intercept=0, name=name)
#   return(model)
# }
#
# sub_df = df[idx_train, c(best_model_cvaglmnet_probes,y_key)]
# m = build_model_from_probes(best_model_cvaglmnet_probes, sub_df, y_key, name="best_model_cvaglmnet")
#
# # build model
# predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTr = sqrt(mean((Ytrain - predTr)^2))
# predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTe = sqrt(mean((Ytest - predTe)^2))
#
# layout(matrix(1:2,1), respect=TRUE)
# plot(Ytrain, predTr, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
# plot(Ytest , predTe, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
```






























## Bootstrap

We build a second model to improve the results compare to the first elastic net model. We find a big overfitting so we decided to build a bootstrap model to avoid the overfitting. At the end of the 500 bootstraps, we have the occurence of each probes appearing in the 500 bootstrapped models. When we have all the occurence, we have to find the perfect occurence to choose all the probes appearing more than this occurence to build our new model. 
To find this occurence, we plot cross validation results for each occurence of probes. We see that, at the beginning, validation RMSE decreases a lot, like nested train RMSE. But at a moment, we "have" an elbow and validation RMSE stagnate when the nested train RMSE keep decreases, which increases the overfitting. So we choose the occurence at the end of the "elbow" to minimize the validation RMSE while avoiding overfitting. This method is called "Elbow method" and is widely used in this kind of situation.
For information, Bootstrap is a method consisting in sample a bootstrap train set based on the original train set. We repete the method a big number of times, more we repete it, more the results are robusts. To create the bootstrapped train set, we sample 2/3 of sample on original train set without replace and 1/3 with replace. That create a bootstrapped train set used to make our model.
We repete this method 500 times and, at the end, we select for our final model only the markers shows on more than a certain percentage on bootstraps models. That create a final model more robusts than if we just make one model on our original train set, so this method increases robustness of results.



### Probe occurence over bootstraps



```{r bootstrap}
bs_func = function(i, idx_train, best_model_cvaglmnet_params, gse, idx_cpg, confounders, USE_PARAPPLY) { #epimedtools to see evolution of running 
  # Bootstrap sample creation
  # print(i)
  set.seed(i)
  # idx_bstrain = idx_train
  idx_bstrain =   c(
    sample(idx_train, ceiling(2/3 * length(idx_train)), replace=FALSE),
    sample(idx_train, floor(  1/3 * length(idx_train)), replace=TRUE)
  )
  # Create model with best meta params and bootstrap sample
  # print(best_model_cvaglmnet_params)
  alpha = best_model_cvaglmnet_params$alpha
  lambda = best_model_cvaglmnet_params$lambda
  # print(length(idx_cpg))
  if (USE_PARAPPLY) {
    models_bs = dnamaging::model_factory_glmnet(idx_train=idx_bstrain, y_key=y_key, alpha=alpha, lambda=lambda, gse=gse, idx_cpg=idx_cpg, confounders=confounders, MEMOISATION=FALSE) 
  } else {
    models_bs = mmodel_factory_glmnet(idx_train=idx_bstrain, y_key=y_key, alpha=alpha, lambda=lambda, gse=gse, idx_cpg=idx_cpg, confounders=confounders)
  }
  # For each bootstrap, capture all probes use in the model 
  probes = rownames(models_bs$coeff)
  return(probes)
}

USE_PARAPPLY = TRUE


if (USE_PARAPPLY) {
  print("bootstrap using parallel::parApply...")
  fetchit = mget_full_cpg_matrix(gse)
  fetchit = mget_df_preproc(gse)
  if (!exists("cl_bs")) {
    if(!exists("nb_core")) {nb_core = parallel::detectCores()}
    cl_bs = parallel::makeCluster(nb_core, type="FORK")
  }
  bs_probes = parallel::parApply(cl_bs, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, best_model_cvaglmnet_params=best_model_cvaglmnet_params, gse=gse, idx_cpg=idx_cpg, confounders=confounders,  USE_PARAPPLY=USE_PARAPPLY)
  parallel::stopCluster(cl_bs)
  rm("cl_bs")
} else {
  USE_PARAPPLY = FALSE
  print("bootstrap using epimedtools::monitored_apply")
  bs_probes = epimedtools::monitored_apply(mod=1, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, best_model_cvaglmnet_params=best_model_cvaglmnet_params, gse=gse, idx_cpg=idx_cpg, confounders=confounders, USE_PARAPPLY=USE_PARAPPLY)   
}

```


```{r robust_probes}
layout(matrix(1:2,1),respect = TRUE)
probes = unlist(bs_probes) # probes of all bootstraps
tmp_tab = table(probes) # distribution of all probes
# robust_probes = names(tmp_tab)[tmp_tab >= n_boot/2] # probes which appears in more than 50% of models
barplot(rev(table(table(probes))), las=2, 
  xlab=paste0("probes occurence (for ", n_boot, " bootstraps)"), 
  ylab="nb. probes",
  main=paste0("Probes occurence") 
) # barplot of occurence of each probes across bootstraps

barplot(cumsum(rev(table(table(probes)))), las=2, 
  xlab=paste0("cumulated probes occurence (for ", n_boot, " bootstraps)"), 
  ylab="nb. probes",
  main=paste0("Cumulated probes occurence") 
) # barplot of occurence of each probes across bootstraps
# abline(h=length(robust_probes), lty=2, col=2)
# text(n_boot/4, length(robust_probes), pos=3, paste0(length(robust_probes), " probes"), col=2)
```


### Robust probes

```{r cv_bs, results="hide"}
tmp_tab = sort(table(unlist(bs_probes)))
best_model_bs_probes = names(tmp_tab)[tmp_tab > (n_boot / 2)]
best_model_bs_probes = intersect(best_model_bs_probes, idx_cpg)
```


```{r best_model_bs_probes, echo=TRUE, results="verbatim"}
best_model_bs_probes
```


```{r best_model_bs}
if (scenario=="E") {
  confounders = "gender01"
} else if (scenario=="F") {
  confounders = c(
    # "B"      ,
    "NK"     ,
    "CD4T"   ,
    "CD8T"   ,
    "Gran"   ,
    "Mono"   ,
    "Neutro" ,
    "Eosino"
  )
} else if (scenario=="G") {
  confounders = c(
    "gender01"      ,
    "NK"     ,
    "CD4T"   ,
    "CD8T"   ,
    "Gran"   ,
    "Mono"   ,
    "Neutro" ,
    "Eosino"
  )
} else {
  # DO NOTHING
}
sub_df = df[idx_train,c(best_model_bs_probes,y_key, confounders)] 
x = as.matrix(sub_df[idx_train, c(best_model_bs_probes, confounders)])
y = sub_df[idx_train, y_key]
m = glmnet::cv.glmnet(x=x, y=y, alpha=0, type.measure="mse", standardize=TRUE)
tmp_alpha = 0
tmp_lambda = m$lambda.min
# tmp_lambda = m$lambda.1se
m = glmnet::glmnet(x=x, y=y, alpha=tmp_alpha, lambda=tmp_lambda, standardize=TRUE)
idx = rownames(m$beta)[m$beta@i+1]
coeff=data.frame(probes=idx, beta=m$beta[idx,])
rownames(coeff) = idx
ret = list(Intercept=m$a0, coeff=coeff)
ret$name = "Bootstrapped model"

best_model_bs = ret
```





```{r plot models_bs, eval=FALSE}
# #Plot rmse like with cross validation but with bootstrap results
# layout(matrix(1:2,1), respect=TRUE)
# m = best_model_bs
# predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTr = sqrt(mean((Ytrain - predTr)^2))
# predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
# rmseTe = sqrt(mean((Ytest - predTe)^2))
# plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " RMSE: ", signif(rmseTr, 3)))
# plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " RMSE: ", signif(rmseTe, 3)))
```

























## Model Evaluation


```{r import/export models}
litterature_models = readRDS("litterature_models.rds")
models = list(
  best_model_cvaglmnet, 
  best_model_bs, 
  litterature_models$hannum_model_mc, 
  litterature_models$horvath_model_mc
)
saveRDS(list(best_model_cvaglmnet, best_model_bs), paste0("models_", gse, ".rds"))
```


```{r model evaluation, fig.width=12, fig.height=15}
#Create Train and Test samples
Xtrain = mget_full_cpg_matrix(gse, idx_train, idx_cpg)
Xtest =  mget_full_cpg_matrix(gse, idx_test,  idx_cpg)
if (!is.null(confounders)) {
  conf_train = mget_df_preproc(gse)[idx_train, confounders]
  Xtrain = cbind(Xtrain, conf_train)            
  Xtrain = as.matrix(Xtrain)
  conf_test = mget_df_preproc(gse)[idx_test, confounders]
  Xtest = cbind(Xtest, conf_test)            
  Xtest = as.matrix(Xtest)
}

Ytrain = mget_df_preproc(gse)[idx_train,y_key]
Ytest =  mget_df_preproc(gse)[idx_test, y_key]

# df$qAge = cut(df[,y_key], quantile(df[,y_key]), include.lowest=TRUE)
# if (!"qAge" %in% covariates) covariates = c(covariates, "qAge")
for (covariate in covariates) {
  layout(matrix(1:20, 5), respect=TRUE)
  # ADD
  info_g = list()
  for (m in models) {
    # layout(matrix(1:5, 1), respect=TRUE)
    info = dnamaging::plot_model_eval(m, df, covariate, Xtrain=Xtrain, Xtest=Xtest, Ytrain=Ytrain, Ytest=Ytest)
    info_g[[length(info_g)+1]] = info
  }
  names(info_g) = c("ElasticNet","Bootstrap","Hannum","Horvath") 
  info_g$ElasticNet$alpha  = best_model_cvaglmnet_params$alpha
  info_g$ElasticNet$lambda = best_model_cvaglmnet_params$lambda 
  
}
```








In this figure, we can see the results for each methods. At the end, we have four models, the ElasticNet model, the Bootstrap model, the Hannum model (Hannum clock) and the Horvath model (Horvath Clock). For the litterature models (Hannum and Horvath), we take probes from their clocks and learning on our dataset with a linear regression. That gives us 2 litterature models usable for our datasets. So, for each model, we have three plots. The first is the predicted age depending on the chronological age for the train set. To see if we have overfitting, we compare this graph with the second graph which plot the predicted age depending on the chronological age for the test set. This second graph shows us the efficiency of our model. For the last graph, we build the AMAR on the test set depending on cofactors to see if cofactors have an impact on AMAR, so in DNAm. The AMAR (Apparent Methylomic Aging Rate) [Hannum,2013] is the factor between predicted age (methylomic age) and chronological age. This measure can show us the impact of cofactors in DNAm. For each model of this dataset except for Horvath, we have an significative different of AMAR between men and women. We use the same dataset as Hannum is 2013 to compare and verify our results. We obtains the same conclusion as him, men and women have significant different AMAR in the GSE40279 dataset. 

We want to see if we can obtain the same results in a different platform. So we transform the 450k dataset using in Hannum paper in a 27k dataset. And we can see in this figure that we always obtain for our 2 self made models a significant difference in AMAR, that we not see in Hannum model, because we have only 7 probes in a 27k platform compare to 71 in a 450k, so this model don’t have enough probes to work. That shows us that our Elasticnet and Bootstrap model are reusable in other platforms and dataset condition of relearn models in new datasets. 







For each alpha, we see the occurence of each probes across bootstraps.


Discussion, a **core of probes** are always selected by bootstrap.

- What is the relevance of the clock built with only this core of probes? #biostat
- Which one? What are associated biologiocal functions? #biologist 
- Are these probes located into wild Differebnttion Metrhylation Region (DMR)? #biologist


## Exporting probe annotations

```{r probe annotation, eval=FALSE}
library(IlluminaHumanMethylation450kanno.ilmn12.hg19)
pf450k = data.frame(getAnnotation(IlluminaHumanMethylation450kanno.ilmn12.hg19))
probes_annot = pf450k[models[[2]]$coeff$probes,]
table(probes_annot$Relation_to_Island)
probes_annot_filename = paste0("04_probes_annot_", gse, ".xlsx")
WriteXLS::WriteXLS(probes_annot, probes_annot_filename)

# candidates = openxlsx::read.xlsx("~/projects/expedition_5300/results/cmslimapuno/vignettes/global_results_study_exp5300_mergemeth_convoluted.rds_modelcalllm_meth~cmslimapuno+tissue_0.01.xlsx", colNames=TRUE);
# candidates[unlist(sapply(models[[1]]$coeff$probes, grep, candidates$probes)),]
# candidates[unlist(sapply(models[[2]]$coeff$probes, grep, candidates$probes)),]
# candidates[unlist(sapply(models[[3]]$coeff$probes, grep, candidates$probes)),]
# candidates[unlist(sapply(models[[4]]$coeff$probes, grep, candidates$probes)),]





# [./`r probes_annot_filename`](`r probes_annot_filename`)
```





























































```{r hannum distribution, eval=FALSE}
# ## Hannum probes
#
# For each alpha, we see the occurence of Hannum probes across bootstraps.

layout(1, respect=TRUE)
# layout(matrix(1:2,1),respect = TRUE)

probes = unlist(bs_probes)
tmp_tab = table(probes)

litterature_models = readRDS("litterature_models.rds")
# hannum_probes = intersect(names(tmp_tab), litterature_models$hannum_model_mc$coeff$probes)
hannum_probes = litterature_models$hannum_model_mc$coeff$probes

tmp_tab_Hannum = tmp_tab[hannum_probes] # Occurence of each Hannum's probes
tmp_tab_Hannum[is.na(tmp_tab_Hannum)] = 0  # If no occurences, set 0 
barplot(table(tmp_tab_Hannum), las=2, main=paste0("Hannum probes in ", sum(tmp_tab_Hannum >= n_boot/2), " probes.")) # Plot occurences of Hannum probes across bootstraps
foo = names(tmp_tab_Hannum)[tmp_tab_Hannum >= n_boot/2]
unique(unlist(foo))
```


```{r probes_occurrence, eval=FALSE}
# ## Occurrence of probes along of boostrap process
#
# Follow the number of retained probes along of the bootstraps process

layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:(length(models)*2),2), respect=TRUE)
x = seq(2,n_boot,2) # Sequence of even bootstraps
tmp_probes = lapply(x, function(nb_boot){ # when the number of bootstrap is even 
  tmp_probes = unlist(bs_probes[1:nb_boot]) 
  tmp_tab = table(tmp_probes)
  tmp_boot_cpg = names(tmp_tab)[tmp_tab >= nb_boot/2] # Check probes appears more than 50% times
  return(tmp_boot_cpg)
})
# barplot(sort(table(unlist(tmp_probes))))
nb_probes = unlist(sapply(tmp_probes, length)) # Check number of probes across boostraps
plot(x, nb_probes, type = "l", main ="") # Plot number of probes across bootstraps
tab_probes = table(nb_probes) 
barplot(tab_probes, las=2) # Check when number of probes stabilised
```






https://github.com/fchuffar/dnamaging


