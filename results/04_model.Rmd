---
title: "Build prediction model"
author: "Fabien Jossaud, Florent Chuffart"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
---




# Biological clocks

Biological clocks are statstical tools allowing to **predict** age according bilogical parameters.

*e.g.*

$$Age \sim telomere size$$ [Ref.]

$$Age \sim composition cellulaire$$ 

$$Age \sim DNAm$$


Note: Predictive model are different than explanatory model.

*e.g.*

15% of 450k probes are correlated to the age,
but only 71 probes are used in Hannum 2013 clock.











Since, we use the clock metaphore, aging characterizes a shift between chronological age and biologiocal (or predicted) age.
But, more than aging, epigenetic clock points out the DNAm plastic part over time.

Then, we use epigenetic clock (predictiv model) to study cofactor effects (altitude,  air pollution...) on DNAm.
Thereby, epigenetic clock becomes a powerfull ligthweight tool to emphasis effects of cofactor on (15% of) DNAm.

*e.g.* 

In Hannum 2013, Aging Methylation Acceleration Rate (AMAR) according do sex shows differential aging.




Lightness of epigenetic clocks allow to treat cases more powerfully than it could be with DMR.

*e.g.*

clock on 27k  vs. DMR on 27k







Hypothèse : etudier la meth au regard de l’age permet de capturer la partie plastique du DNAm











 

```{r echo=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment = "#>", fig.width=9, fig.height=6, eval=TRUE, echo=TRUE, results="verbatim")
start_time = Sys.time()
source("common.R")
```

```{r params_default, echo=FALSE}
source("params_default.R")
```

```{r building_indexes, echo=FALSE}
idx_samples = rownames(df)
markers_start = grep("cg",colnames(df))[1]
idx_clinicals = colnames(df)[1:(markers_start-1)]
idx_cpg = colnames(df)[markers_start:ncol(df)]
# services patterns
if (!exists("mget_df")) {mget_df = memoise::memoise(function(){df})}
if (!exists("mget_full_cpg_matrix")) {mget_full_cpg_matrix = memoise::memoise(function(idx_smp){as.matrix(mget_df()[idx_smp,idx_cpg])}, cache = cachem::cache_mem(max_size = 10*1024 * 1024^2))}
```

# Parameters 

```{r parameters}
#Define train and test indexes
# nb_train = floor(nrow(df)/2)
nb_train = 482
set.seed(1)
idx_train = sample(rownames(df), nb_train)
idx_test = setdiff(rownames(df), idx_train)

# cross-validation
if(!exists("alphas")) alphas = c(.1, .15, .2, .25, .5, 1)

# boostrap 
if (!exists("n_boot")) n_boot = 500
``` 


# Cross-Validation 

Find best meta parameters with cross validation

#datascientist

```{r glmnet::cv.glmnet and glmnetUtils::cva.glmnet memoisation, echo=FALSE}
### Memoisation glmnet::cv.glmnet call
if (!exists("mcvglmnet")) {
  cvglmnet = function(alpha, idx_train, y_key) {
    x = mget_full_cpg_matrix(idx_train)
    y = mget_df()[idx_train, y_key] # service. Design Oatterns, Gamma et al. 
    modelcv = glmnet::cv.glmnet(x=x, y=y, alpha=alpha, type.measure="mse", standardize=TRUE)
    cvrecap = data.frame(lambda=modelcv$lambda, rmse=sqrt(modelcv$cvm), nbprobes=modelcv$nzero, alpha=alpha)
    return(cvrecap)  
  }
  mcvglmnet = memoise::memoise(cvglmnet)
  if (exists("cl")) {
    parallel::stopCluster(cl)
  }
  if (!exists("cl")) {
    nb_cores = parallel::detectCores()
    cl <<- parallel::makeCluster(nb_cores,  type="FORK")
    # parallel::stopCluster(cl)
  }
} # Memoise for 

if (!exists("mcvaglmnet")) {
  cvaglmnet = function(idx_train, y_key) {
    x = mget_full_cpg_matrix(idx_train)
    y = mget_df()[idx_train, y_key] # service. Design Oatterns, Gamma et al. 
    cl.cva = parallel::makeCluster(nb_cores,  type="FORK")
    modelcva = glmnetUtils::cva.glmnet(x=x, y=y, type.measure="mse", standardize=TRUE, outerParallel = cl.cva)
    parallel::stopCluster(cl.cva)
    # cvrecap = data.frame(lambda=modelcv$lambda, rmse=sqrt(modelcv$cvm), nbprobes=modelcv$nzero, alpha=alpha)
    return(modelcva)  
  }
  mcvaglmnet = memoise::memoise(cvaglmnet)
} # Memoise for cvaglmnet
```


```{r glmnet::cv.glmnet call}
cvrecaps = lapply(alphas, function(alpha, idx_train=idx_train, y_key=y_key) { #list with cv recap (lambda,rmse,nbprobes) and lambda.min  
  print(alpha)
  cvrecap = mcvglmnet(alpha=alpha, idx_train=idx_train, y_key=y_key)
  return(cvrecap)  
}, idx_train=idx_train, y_key=y_key)
cvrecaps = do.call(rbind, cvrecaps)
head(cvrecaps)
dim(cvrecaps)
```

```{r cva.glmnet call, eval=TRUE, echo=TRUE}
modelcva = mcvaglmnet(idx_train=idx_train, y_key=y_key)

length(modelcva)
cvarecaps = data.frame(
  lambda   = unlist(lapply(modelcva$modlist, "[[", "lambda")), 
  cvm      = unlist(lapply(modelcva$modlist, "[[", "cvm")   ), 
  nbprobes = unlist(lapply(modelcva$modlist, "[[", "nzero") ),
  alpha    = rep(modelcva$alpha, sapply(lapply(modelcva$modlist, "[[", "lambda"), length))
)
cvarecaps$rmse = sqrt(cvarecaps$cvm)
head(cvarecaps)
dim(cvarecaps)

if (exists("cvrecaps")) {
  cvrecaps = rbind(cvrecaps, cvarecaps[, colnames(cvrecaps)])
} else {
  cvrecaps = cvarecaps  
}
```


**Need to add error bar**

```{r plot cvglmnet results, echo=FALSE}
main = paste0("Cross-Validation nb_train=", nb_train)
# Plotting rmse and nbprobes depending on alpha and lambda (with cv results)
layout(matrix(1:2,1),respect = TRUE)
plot(0, 0, col=0, xlab="log10(lambda)", ylab="RMSE", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, min(cvrecaps$rmse)^2))
tmp_alphas = sort(unique(cvrecaps$alpha))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$rmse             , col=adjustcolor(tmp_col, alpha.f=1), lty=2)
  # abline(v=log10(sub_recap$lambda)                      , col=adjustcolor(tmp_col, alpha.f=1), lty=2)
  points(  log10(sub_recap$lambda), sub_recap$rmse  , col=tmp_col, pch=16)
})
plot(0, 0, col=0, xlab="log10(lambda)", ylab="nbprobes", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, ceiling(nb_train)))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$nbprobes         , col=adjustcolor(tmp_col, alpha.f=.5), lty=2)
  # abline(v=log10(sub_recap$lambda)                      , col=adjustcolor(tmp_col, alpha.f=.5), lty=2)
  points(  log10(sub_recap$lambda), sub_recap$nbprobes  , col=tmp_col, pch=16)
})
legend(x="topright", legend=tmp_alphas, fill=1:length(tmp_alphas), title="alpha", cex=.5)
```


```{r best model}
best_model = cvrecaps[cvrecaps$rmse==min(cvrecaps$rmse),][1,]
best_model

alpha = best_model[["alpha"]]
lambda = best_model[["lambda"]]
m_params = c(alpha=alpha, lambda=lambda)
model_params = data.frame(alpha=alpha, lambda=lambda)
rownames(model_params) = paste0("glmnet_a", signif(alpha,3), "_l", signif(lambda,3))
```

```{r loading train/test sets}
#Create Train and Test samples
Xtrain = mget_full_cpg_matrix(idx_train)
Ytrain = mget_df()[idx_train,y_key]
Xtest = mget_full_cpg_matrix(idx_test)
Ytest = mget_df()[idx_test,y_key]
``` 


```{r glmnet::glmnet memoisation, echo=FALSE}
# model_factory_glmnet, it produces custom model object based on list by calling glmnet::glmnet function
if (!exists("mmodel_factory_glmnet")) {
  model_factory_glmnet = function(idx_train, y_key, alpha=alpha, lambda=lambda) {
    x = mget_full_cpg_matrix(idx_train)
    y = mget_df()[idx_train,y_key]
    m = glmnet::glmnet(x=x, y=y, alpha=alpha, lambda=lambda, standardize=TRUE) 
    idx = rownames(m$beta)[m$beta@i+1]
    coeff=data.frame(probes=idx, beta=m$beta[idx,])
    rownames(coeff) = idx
    coeff$mean = apply(x[,rownames(coeff)], 2, mean)
    ret = list(Intercept=m$a0, coeff=coeff)
    ret$name = paste0("glmnet_", signif(alpha,3), "_", signif(lambda,3))
    # ret$glmmod = m
    return(ret)
  }
  mmodel_factory_glmnet = memoise::memoise(model_factory_glmnet, cache = cachem::cache_mem(max_size = 10*1024 * 1024^2))
}
```

```{r prediction for best model}
# build model
alpha = m_params[["alpha"]]
lambda = m_params[["lambda"]]
m = mmodel_factory_glmnet(idx_train=idx_train, y_key=y_key, alpha=alpha, lambda=lambda) 
# prediction 
predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
rmseTr = sqrt(mean((Ytrain - predTr)^2))
predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
rmseTe = sqrt(mean((Ytest - predTe)^2))
```

```{r plot prediction for best model, echo=FALSE}
layout(matrix(1:2,1), respect=TRUE)
plot(Ytrain, predTr, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
plot(Ytest , predTe, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " test  rmse: ", signif(rmseTe, 3))) 
```

















```{r glmnet_build_deprecated, echo=FALSE, eval=FALSE}
# ## Model evaluation with best meta parameters
# # best model_params = list(alphas, lambdas)
# model_params = data.frame(t(sapply(alphas, function(alpha, cvrecaps=cvrecaps) {
#   cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
#   # lambda = cvrecap[abs(350-cvrecap$nbprobes)==min(abs(350-cvrecap$nbprobes)),]$lambda[1]
#   lambda = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$lambda[1]
#   ret = c(alpha=alpha, lambda=lambda)
#   return(ret)
# }, cvrecaps=cvrecaps)))
# # model_params
#
#
# ## Create models
#
# models = apply(model_params, 1, function(m_params, idx_train=idx_train, y_key=y_key) {
#   # print(m_params)
#   alpha = m_params[["alpha"]]
#   lambda = m_params[["lambda"]]
#   m = mmodel_factory_glmnet(idx_train=idx_train, y_key=y_key, alpha=alpha, lambda=lambda)
#   return(m)
# }, idx_train=idx_train, y_key=y_key)
# names(models) = sapply(models, "[[", "name")
#
#
#
# ## Plot predictions with train an test set of each models
#
# #Create Train and Test samples
# Xtrain = mget_full_cpg_matrix(idx_train)
# Ytrain = mget_df()[idx_train,y_key]
# Xtest = mget_full_cpg_matrix(idx_test)
# Ytest = mget_df()[idx_test,y_key]
#
# layout(matrix(1:2,1), respect=TRUE)
# # layout(matrix(1:(length(models)*2),2), respect=TRUE)
# foo = lapply(models, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {
#   predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
#   rmseTr = sqrt(mean((Ytrain - predTr)^2))
#   predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
#   rmseTe = sqrt(mean((Ytest - predTe)^2))
#   plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
#   plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
# }, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```


# Bootstrap

What is bootstrap?

Boostrap allows to increase robustness of results.

#datascientist

```{r boostrap}
bs_func = function(i, idx_train, model_params) { #epimedtools to see evolution of running 
  # Bootstrap sample creation
  # print(i)
  set.seed(i)
  idx_bstrain = c(
    sample(idx_train, ceiling(2/3 * length(idx_train)), replace=FALSE),
    sample(idx_train, floor(  1/3 * length(idx_train)), replace=TRUE)
  )
  # Create model with best meta params and bootstrap sample
  models_bs = apply(model_params, 1, function(m_params, idx_bstrain=idx_bstrain, y_key=y_key) { 
    # print(m_params)
    alpha = m_params[["alpha"]]
    lambda = m_params[["lambda"]]
    m = mmodel_factory_glmnet(idx_bstrain, y_key, alpha=alpha, lambda=lambda) 
    return(m)  
  }, idx_bstrain=idx_bstrain, y_key=y_key)
  names(models_bs) = paste0(rownames(model_params), "_rnd", i)
  # For each bootstrap, capture all probes use in the model 
  probes = lapply(models_bs, function(m) { 
    probes = rownames(m$coeff)
    return(probes)
  })
  names(probes) = names(models_bs)
  bs = list(probes=probes)
  return(bs)
}

USE_PARAPPLY = FALSE

if (USE_PARAPPLY) {
  print("bootstrap using parallel::parApply...")
  if (!exists("cl_bs")) {
    cl_bs = parallel::makeCluster(parallel::detectCores(),  type="PSOCK")    
    # parallel::stopCluster(cl_bs)
  }
  bs = parallel::parApply(cl_bs, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, model_params=model_params, mmodel_factory_glmnet=mmodel_factory_glmnet)
} else {
  print("bootstrap using epimedtools::monitored_apply")
  bs = epimedtools::monitored_apply(mod=1, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, model_params=model_params)   
}

bs_probes = unlist(unlist(bs, recursive=FALSE), recursive=FALSE)
length(bs_probes)

# bs_probes = unlist(lapply(bs, "[[", "probes"), recursive=FALSE)
# bs_probes = lapply(bs, "[[", "probes")
# bs_models = bs$models
```


Put there predisction train and test like for CV




# Results and Discussion

For each alpha, we see the occurence of each probes across bootstraps.


Discussion, a **core of probes** are always selected by bootstrap.

- What is the relevance of the clock built with only this core of probes? #biostat
- Which one? What are associated biologiocal functions? #biologist 
- Are these probes located into wild Differebnttion Metrhylation Region (DMR)? #biologist

```{r bootstrap distribution}
layout(1, respect=TRUE)
layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:4,1),respect = TRUE)
boot_probes = lapply(1:nrow(model_params),function(i){
  #  probes = lapply(bs_probes, "[[", i) #Probes for each alpha
  probes = unlist(bs_probes) # probes of all bootstraps
  tmp_tab = table(probes) # distribution of all probes
  tmp_boot_probes = names(tmp_tab)[tmp_tab >= n_boot/2] # probes which appears in more than 50% of models
  barplot(table(table(probes)), las=2, 
    xlab="nb of occurence", 
    sub = paste0("keep ", length(tmp_boot_probes), " probes"),
    main=paste0("Probes occurence distribution for ", rownames(model_params)[i]) 
  ) # barplot of occurence of each probes across bootstraps

  barplot(cumsum(rev(table(table(probes)))), las=2, 
    xlab="nb of cum. occurence", 
    sub = paste0("keep ", length(tmp_boot_probes), " probes"),
    main=paste0("Cumulated probes occurence distribution for ", rownames(model_params)[i]) 
  ) # barplot of occurence of each probes across bootstraps
  
  
  
  return(tmp_boot_probes)
})
names(boot_probes) = paste0("bs_", rownames(model_params))
```

## Hannum probes

For each alpha, we see the occurence of Hannum probes across bootstraps.

```{r hannum distribution}
layout(1, respect=TRUE)
# layout(matrix(1:2,1),respect = TRUE)
if (!exists("mget_coefHannum")) mget_coefHannum = memoise::memoise(methylclockData::get_coefHannum)
hannum_coeff = mget_coefHannum() # Get Hannum probes
foo = lapply(1:nrow(model_params), function(i) {
  # probes = lapply(bs_probes, "[[", i)
  probes = unlist(bs_probes)
  tmp_tab = table(probes)
  tmp_tab_Hannum = tmp_tab[hannum_coeff$CpGmarker] # Occurence of each Hannum's probes
  tmp_tab_Hannum[is.na(tmp_tab_Hannum)] = 0  # If no occurences, set 0 
  barplot(table(tmp_tab_Hannum), las=2, main=paste0("Hannum probes in ", rownames(model_params)[i], " ", sum(tmp_tab_Hannum >= n_boot/2), " probes.")) # Plot occurences of Hannum probes across bootstraps
  ret = names(tmp_tab_Hannum)[tmp_tab_Hannum >= n_boot/2]
  return(ret)
})
unique(unlist(foo))
```

## Occurrence of probes along of boostrap process

Follow the number of retained probes along of the bootstraps process

```{r probes_occurrence }
layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:(length(models)*2),2), respect=TRUE)
x = seq(2,n_boot,2) # Sequence of even bootstraps
for (i in 1:nrow(model_params)){ #for each alpha
  tmp_probes = lapply(x, function(nb_boot){ # when the number of bootstrap is even 
    tmp_probes = unlist(bs_probes[1:nb_boot]) 
    tmp_tab = table(tmp_probes)
    tmp_boot_cpg = names(tmp_tab)[tmp_tab >= nb_boot/2] # Check probes appears more than 50% times
    return(tmp_boot_cpg)
  })
  # barplot(sort(table(unlist(tmp_probes))))
  nb_probes = unlist(sapply(tmp_probes, length)) # Check number of probes across boostraps
  plot(x, nb_probes, type = "l", main = paste0("alpha: ", rownames(model_params)[i])) # Plot number of probes across bootstraps
  tab_probes = table(nb_probes) 
  barplot(tab_probes, las=2) # Check when number of probes stabilised
}
# tab_probes
```























```{r stock bootstrap model}
tmp_idx_probes = unique(unlist(boot_probes))
tmp_train = df[idx_train,c(tmp_idx_probes,y_key)] # train set with CpG and y_key for lm
models_bs = lapply(1:length(boot_probes), function(i){ # Compute coefficients and mean in the train for each probes of the model for each alpha
	m = lm(formula(paste0(y_key,"~",paste0(boot_probes[[i]],collapse="+"))),data=tmp_train) #lm model with probes previously find with bootstrap
  idx = boot_probes[[i]]
  coeff = data.frame(probes=idx, beta=m$coefficients[idx])
  rownames(coeff) = idx
  coeff$mean = apply(tmp_train[,rownames(coeff)], 2, mean)
  head(coeff)
  ret = list(Intercept=m$coefficients[[1]], coeff=coeff)
  ret$name = names(boot_probes)[i]
  # ret$glmmod = m
  return(ret)
})
names(models_bs) = sapply(models_bs, "[[", "name")
```

```{r rmse model}
#Plot rmse like with cross validation but with bootstrap results
layout(matrix(1:2,1), respect=TRUE)
# layout(matrix(1:8,2), respect=TRUE)
foo = lapply(models_bs, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {               
  predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTr = sqrt(mean((Ytrain - predTr)^2))
  predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTe = sqrt(mean((Ytest - predTe)^2))
  plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
  plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))  
}, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```


# AMAR


```{r build hannum model, eval=FALSE}
litterature_models = list()

foo = mget_coefHannum # Get Hannum probes
hannum_coeffs = foo$CoefficientTraining
hannum_probes = foo$CpGmarker
# Build Hannum model with methylclock coeffs 
idx = hannum_probes
coeff = data.frame(probes=idx, beta=hannum_coeffs)
rownames(coeff) = idx
coeff$mean = apply(df[idx_train,rownames(coeff)], 2, mean)
head(coeff)
hannum_model_mc = list(Intercept=0, coeff=coeff)
hannum_model_mc$name = paste0("hannum_model_mc")
litterature_models[["hannum_model_mc"]] = hannum_model_mc

m = lm(formula(paste0(y_key,"~0+",paste0(hannum_probes,collapse="+"))),data=df[idx_train,]) #lm model with probes previously find with bootstrap
idx = hannum_probes
coeff = data.frame(probes=idx, beta=m$coefficients[idx])
rownames(coeff) = idx
coeff$mean = apply(df[idx_train,rownames(coeff)], 2, mean)
head(coeff)
hannum_model_dflm = list(Intercept=0, coeff=coeff)
hannum_model_dflm$name = "hannum_model_dflm"
litterature_models[["hannum_model_dflm"]] = hannum_model_dflm

foo = methylclockData::get_coefHorvath() # Get Hannum probes
horvath_coeffs = foo$CoefficientTraining[-1]
horvath_probes = foo$CpGmarker[-1]
horvath_median = foo$medianByCpG[-1]
horvath_intercept = foo$CoefficientTraining[1]
# Build Hannum model with methylclock coeffs 
idx = horvath_probes
coeff = data.frame(probes=idx, beta=horvath_coeffs)
rownames(coeff) = idx
coeff$mean = horvath_median
head(coeff)
horvath_model_mc = list(Intercept=horvath_intercept, coeff=coeff)
horvath_model_mc$name = paste0("horvath_model_mc")
litterature_models[["horvath_model_mc"]] = horvath_model_mc

m = lm(formula(paste0(y_key,"~0+",paste0(horvath_probes,collapse="+"))),data=df[idx_train,]) #lm model with probes previously find with bootstrap
idx = horvath_probes
coeff = data.frame(probes=idx, beta=m$coefficients[idx])
rownames(coeff) = idx
coeff$mean = apply(df[idx_train,rownames(coeff)], 2, mean)
head(coeff)
horvath_model_dflm = list(Intercept=0, coeff=coeff)
horvath_model_dflm$name = "horvath_model_dflm"
litterature_models[["horvath_model_dflm"]] = horvath_model_dflm


layout(matrix(1:8,2), respect=TRUE)

foo = lapply(litterature_models, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {               
  predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTr = sqrt(mean((Ytrain - predTr)^2))
  predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTe = sqrt(mean((Ytest - predTe)^2))
  plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
  plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))  
}, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)


# # predic ages with mc_DNAmAge
# if (!exists("mmethylclock_DNAmAge")) {mmethylclock_DNAmAge = memoise::memoise(methylclock::DNAmAge)}
# modelHOTr = mmethylclock_DNAmAge(t(Xtrain), age=Ytrain, cell.count=FALSE, clocks="Hannum")
# modelHO = mmethylclock_DNAmAge(t(Xtest),age=Ytest,cell.count = FALSE, clocks = "Hannum")
#
# rmseHOTr = signif(sqrt(mean((modelHOTr$age - modelHOTr$Hannum)^2)),3)
# rmseHO = signif(sqrt(mean((modelHO$age - modelHO$Hannum)^2)),3)
#
# plot(modelHOTr$Hannum, modelHOTr$age, xlab="Methylomic Age", ylab="Chronological Age", main=paste0("Pred with Hannum Method on Train Set, rmse = ",rmseHOTr))
# plot(modelHO$Hannum, modelHO$age, xlab="Methylomic Age", ylab="Chronological Age", main=paste0("Pred with Hannum Method on Test Set, rmse = ",rmseHO))
#
# predTr = modelHOTr$Hannum
# Ytrain == modelHOTr$age
# rmseTr = sqrt(mean((Ytrain - predTr)^2))
# predTe = modelHO$Hannum
# Ytest == modelHO$age
# rmseTe = sqrt(mean((Ytest - predTe)^2))
# plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
# plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))

```






# Session Information

```{r, results="verbatim"}
end_time = Sys.time()
print(paste0("Execution time for vignette : ", end_time - start_time))
sessionInfo()
```
