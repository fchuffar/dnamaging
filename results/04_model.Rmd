---
title: "Build prediction model"
author: "Fabien Jossaud, Florent Chuffart"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
---


```{r echo=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment = "#>", fig.width=9, fig.height=6, eval=TRUE, echo=TRUE, results="verbatim")
source("common.R")
```

```{r params}
source("params_default.R")
```

```{r building_indexes}
start_time = Sys.time()
idx_samples = rownames(df)
markers_start = grep("cg",colnames(df))[1]
idx_clinicals = colnames(df)[1:(markers_start-1)]
idx_cpg = colnames(df)[markers_start:ncol(df)]
```

# Prerequisits

We need to execute 03_preproc to have our df preprocess. 


# Define train and test samples

```{r train/test}
Ntrain = floor(nrow(df)/2)
set.seed(1)
idx_train = sample(rownames(df), Ntrain)
idx_test = setdiff(rownames(df), idx_train)
```

# Model 

```{r prepare model}
tmp_mat_cpg = as.matrix(df[,idx_cpg])
# head(tmp_mat_cpg)
Xtrain = tmp_mat_cpg[idx_train,]
Ytrain = df[idx_train,y_key]
Xtest = tmp_mat_cpg[idx_test,]
Ytest = df[idx_test,y_key]
``` 

```{r glmnet model}
## Cross validation for best lambda and alpha
alphas = c(0.1, 0.25, 0.5, 1)


if (!exists("mcv_glmnet")) {mcv_glmnet = memoise::memoise(glmnet::cv.glmnet)}
stat = lapply(alphas, function(alpha) {   
  print(alpha)
  modelcv = mcv_glmnet(Xtrain, Ytrain, alpha=alpha, type.measure="mse", standardize=TRUE)
  lambdamin = modelcv$lambda.min
  cvrecap = data.frame(lambda=modelcv$lambda, rmse=sqrt(modelcv$cvm), nbprobes=modelcv$nzero, alpha=alpha)
  return(cvrecap)  
})
stat = do.call(rbind, stat)
```

# Meta parameters

```{r meta parameters}
## Plot
layout(matrix(1:2,1),respect = TRUE)
plot(0, 0, col=0, xlab="log10(lambda)", ylab="RMSE", xlim=log10(range(stat$lambda)), ylim=range(stat$rmse))
lapply(alphas, function(alpha) { 
  cvrecap = stat[stat$alpha==alpha,]
  points(log10(cvrecap$lambda), cvrecap$rmse, col=which(alpha==alphas))
  abline(v=log10(cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$lambda), col=which(alpha==alphas))
})
legend(x="topleft", legend=alphas, fill=1:length(alphas), title="Alpha")
plot(0, 0, col=0, xlab="log10(lambda)", ylab="nbprobes", xlim=log10(range(stat$lambda)), ylim=range(stat$nbprobes))
lapply(alphas, function(alpha) { 
  cvrecap = stat[stat$alpha==alpha,]
  points(log10(cvrecap$lambda), cvrecap$nbprobes, col=which(alpha==alphas))
  abline(v=log10(cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$lambda), col=which(alpha==alphas))
})
legend(x="topright", legend=alphas, fill=1:length(alphas), title="Alpha")
```

## Model evaluation

```{r model eval}
# With lambda min 
#model_params = lapply(alphas, function(alpha, stat=stat) { 
#  cvrecap = stat[stat$alpha==alpha,]
#  lambda = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$lambda[1]
#  ret = list(alpha=alpha, lambda=lambda)
#  return(ret)
#}, stat=stat)

# With lambda based on nb_probes 
model_params = lapply(alphas, function(alpha, stat=stat) { 
  cvrecap = stat[stat$alpha==alpha,]
  lambda = cvrecap[abs(350-cvrecap$nbprobes)==min(abs(350-cvrecap$nbprobes)),]$lambda[1]
  ret = list(alpha=alpha, lambda=lambda)
  return(ret)
}, stat=stat)

if (!exists("mglmnet")) {
  mglmnet = memoise::memoise(function(...) {
    glmnet::glmnet(...)
  }, cache = cachem::cache_mem(max_size = 10*1024 * 1024^2))
}

model_factory_glmnet = function(Xtrain, Ytrain, alpha=alpha, lambda=lambda) {
  m = mglmnet(Xtrain, Ytrain, alpha=alpha, lambda=lambda, standardize=TRUE) 
  m$name = paste0("glmnet_", signif(alpha,3), "_", signif(lambda,3))
  return(m)
}

models = lapply(model_params, function(m_params, Xtrain=Xtrain, Ytrain=Ytrain) { 
  print(m_params)
  alpha = m_params$alpha
  lambda = m_params$lambda
  m = model_factory_glmnet(Xtrain, Ytrain, alpha=alpha, lambda=lambda) 
  return(m)  
}, Xtrain=Xtrain, Ytrain=Ytrain)
names(models) = sapply(models, "[[", "name")

layout(matrix(1:2,1), respect=TRUE)
#layout(matrix(1:8,2), respect=TRUE)
foo = lapply(models, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) { 
                         
  predTr = predict(m, Xtrain)
  rmseTr = sqrt(mean((Ytrain - predTr)^2))
  predTe = predict(m, Xtest)
  rmseTe = sqrt(mean((Ytest - predTe)^2))

  plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
  plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))  
}, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```


```{r other_models}
# model_params = list(
#   list(alpha=0.25, lambda=2),
#   list(alpha=0.25, lambda=1),
#   list(alpha=0.25, lambda=0.1),
#   NULL
# )
```




# Bootstrap

```{r boostrap}
## bootstrap Cross validation for best lambda and alpha
n_boot = 300


bs_probes = epimedtools::monitored_apply(mod=1, t(t(1:n_boot)), 1, function(i) { # Bootstrap sample creation
  set.seed(i)
  boot_ind = c(
    sample(idx_train, ceiling(2/3 * length(idx_train)), replace=FALSE),
    sample(idx_train, floor(  1/3 * length(idx_train)), replace=TRUE)
  )
  Xtrain = tmp_mat_cpg[boot_ind,]
  Ytrain = df[boot_ind,y_key]

  models = lapply(model_params, function(m_params, Xtrain=Xtrain, Ytrain=Ytrain) { 
    # print(m_params)
    alpha = m_params$alpha
    lambda = m_params$lambda
    m = model_factory_glmnet(Xtrain, Ytrain, alpha=alpha, lambda=lambda) 
    return(m)  
  }, Xtrain=Xtrain, Ytrain=Ytrain)

  probes = lapply(models, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) { 
    probes = rownames(m$beta)[m$beta@i+1]
    return(probes)
  })
  names(probes) = names(models)
  probes
})
```


For each alpha, we see the occurence of each probes across bootstraps.

```{r bootstrap distribution}
layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:4,1),respect = TRUE)
boot_probes = lapply(1:length(alphas),function(i){
  probes = lapply(bs_probes, "[[", i)
  probes = unlist(probes)
  tmp_tab = table(probes)
  tmp_boot_probes = names(tmp_tab)[tmp_tab >= n_boot/2]
  barplot(table(table(probes)), las=2, main=paste0("alpha: ", alphas[i]))
  return(tmp_boot_probes)
})
```

For each alpha, we see the occurence of Hannum probes across bootstraps.

```{r hannum distribution}
layout(matrix(1:2,1),respect = TRUE)
Hannum_Coeff = methylclockData::get_coefHannum()
foo = lapply(1:length(alphas), function(i) {
  probes = lapply(bs_probes, "[[", i)
  probes = unlist(probes)
  tmp_tab = table(probes)
  tmp_tab_Hannum = tmp_tab[Hannum_Coeff$CpGmarker]
  tmp_tab_Hannum[is.na(tmp_tab_Hannum)] = 0  
  barplot(table(tmp_tab_Hannum), las=2, main=paste0("alpha=", alphas[i], ": ", sum(tmp_tab_Hannum >= n_boot/2), " probes."))
  ret = names(tmp_tab_Hannum)[tmp_tab_Hannum >= n_boot/2]
  return(ret)
})
unique(unlist(foo))
```


Follow the number of retained probes along of the bootstraps process

```{r probes number}
layout(matrix(1:2,1),respect = TRUE)
x = seq(2,n_boot,2) 
for (i in 1:length(alphas)){
    tmp_probes = lapply(x, function(nb_boot){ 
      tmp_probes = lapply(bs_probes[1:nb_boot], "[[", i)
      tmp_probes = unlist(tmp_probes)
      tmp_tab = table(tmp_probes)
      tmp_boot_cpg = names(tmp_tab)[tmp_tab >= nb_boot/2]
      return(tmp_boot_cpg)
    })
    # barplot(sort(table(unlist(tmp_probes))))
    nb_probes = unlist(sapply(tmp_probes, length))
    plot(x, nb_probes, type = "l", main = paste0("alpha: ",alphas[i]))
    tab_probes = table(nb_probes)
    barplot(tab_probes, las=2)
}
```

```{r stock bootstrap model}
train = df[idx_train,c(idx_cpg,y_key)] 
model_boot_coeff = lapply(1:length(alphas), function(i){
	m_boot = lm(formula(paste0(y_key,"~",paste0(boot_probes[[i]],collapse="+"))),data=train)
	mean_probes = as.numeric(c("NA",unlist(lapply(boot_probes[[i]], function(j){
		return(mean(train[,j]))
	}))))
	boot_tab = cbind(coefficient = m_boot$coefficients,mean_probes)
	return(boot_tab)
})
```

```{r rmse model}
layout(matrix(1:2,1), respect=TRUE)
#layout(matrix(1:8,2), respect=TRUE)
foo = lapply(model_boot_coeff, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {               
  predTr = Xtrain[,rownames(m)[-1]] %*% as.matrix(m[-1,"coefficient"]) + m[1,"coefficient"]
  rmseTr = sqrt(mean((Ytrain - predTr)^2))
  predTe = Xtest[,rownames(m)[-1]] %*% as.matrix(m[-1,"coefficient"]) + m[1,"coefficient"]
  rmseTe = sqrt(mean((Ytest - predTe)^2))

  plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(" train rmse: ", signif(rmseTr, 3)))
  plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(" test  rmse: ", signif(rmseTe, 3)))  
}, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```

```{r}
end_time = Sys.time()
print(paste0("Execution time for vignette : ", end_time - start_time))
```



# Session Information

```{r, results="verbatim"}
sessionInfo()
```
