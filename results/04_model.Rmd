---
title: "Build prediction model"
author: "Fabien Jossaud, Florent Chuffart"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
---




# Biological clocks

Biological clocks are statstical tools allowing to **predict** age according bilogical parameters.

*e.g.*

$$Age \sim telomere size$$ [Ref.]

$$Age \sim composition cellulaire$$ 

$$Age \sim DNAm$$


Note: Predictive model are different than explanatory model.

*e.g.*

15% of 450k probes are correlated to the age,
but only 71 probes are used in Hannum 2013 clock.











Since, we use the clock metaphore, aging characterizes a shift between chronological age and biologiocal (or predicted) age.
But, more than aging, epigenetic clock points out the DNAm plastic part over time.

Then, we use epigenetic clock (predictiv model) to study cofactor effects (altitude,  air pollution...) on DNAm.
Thereby, epigenetic clock becomes a powerfull ligthweight tool to emphasis effects of cofactor on (15% of) DNAm.

*e.g.* 

In Hannum 2013, Aging Methylation Acceleration Rate (AMAR) according do sex shows differential aging.




Lightness of epigenetic clocks allow to treat cases more powerfully than it could be with DMR.

*e.g.*

clock on 27k  vs. DMR on 27k







Hypothèse : etudier la meth au regard de l’age permet de capturer la partie plastique du DNAm











 

```{r echo=FALSE}
knitr::opts_chunk$set(collapse=TRUE, comment = "#>", fig.width=9, fig.height=6, eval=TRUE, echo=TRUE, results="verbatim")
start_time = Sys.time()
source("common.R")
```

```{r params_default, echo=FALSE}
source("params_default.R")
```

```{r building_indexes, echo=FALSE}
idx_samples = rownames(df)
markers_start = grep("cg",colnames(df))[1]
idx_clinicals = colnames(df)[1:(markers_start-1)]
idx_cpg = colnames(df)[markers_start:ncol(df)]
# services patterns
if (!exists("mget_df")) {mget_df = memoise::memoise(function(){df})}
if (!exists("mget_full_cpg_matrix")) {
  get_full_cpg_matrix = function(idx_smp){as.matrix(mget_df()[idx_smp,idx_cpg])}
  mget_full_cpg_matrix = memoise::memoise(get_full_cpg_matrix, cache = cachem::cache_mem(max_size = 10*1024 * 1024^2))
}
```

# Parameters 

```{r parameters}
#Define train and test indexes
# nb_train = floor(nrow(df)/2)
nb_train = 482
set.seed(1)
idx_train = sample(rownames(df), nb_train)
idx_test = setdiff(rownames(df), idx_train)

# cross-validation
if(!exists("alphas")) alphas = c(.1, .15, .2, .25, .5, 1)

# boostrap 
if (!exists("n_boot")) n_boot = 500
``` 


# Elastic net regularization

Elastic net regularization is...

Meta-parameters are fixed using cross validation.

#datascientist


```{r glmnet::cv.glmnet call deprecated, eval=FALSE, echo=FALSE}
# ### Memoisation glmnet::cv.glmnet call
# if (!exists("mcvglmnet")) {
#   cvglmnet = function(alpha, idx_train, y_key) {
#     x = mget_full_cpg_matrix(idx_train)
#     y = mget_df()[idx_train, y_key] # service. Design Oatterns, Gamma et al.
#     modelcv = glmnet::cv.glmnet(x=x, y=y, alpha=alpha, type.measure="mse", standardize=TRUE)
#     # cvrecap = data.frame(lambda=modelcv$lambda, rmse=sqrt(modelcv$cvm), nbprobes=modelcv$nzero, alpha=alpha)
#     return(modelcv)
#   }
#   mcvglmnet = memoise::memoise(cvglmnet)
# } # Memoise for
#
# modelcv = lapply(alphas, function(alpha){
#   tmp_modelcv = mcvglmnet(idx_train=idx_train, y_key=y_key,alpha)
#   return(tmp_modelcv)
# })
#
# length(modelcv)
# cvrecaps = data.frame(
#   lambda   = unlist(lapply(modelcv, "[[", "lambda")),
#   cvm      = unlist(lapply(modelcv, "[[", "cvm")   ),
#   cvup     = unlist(lapply(modelcv, "[[", "cvup")  ),
#   cvlo     = unlist(lapply(modelcv, "[[", "cvlo")  ),
#   nbprobes = unlist(lapply(modelcv, "[[", "nzero") ),
#   alpha    = rep(alphas, sapply(lapply(modelcv, "[[", "lambda"), length))
# )
# cvrecaps$rmse = sqrt(cvrecaps$cvm)
# cvrecaps$rmseup = sqrt(cvrecaps$cvup)
# cvrecaps$rmselo = sqrt(cvrecaps$cvlo)
# head(cvrecaps)
# dim(cvrecaps)
#
# # if (exists("cvrecaps")) {
# #   cvrecaps = rbind(cvrecaps, cvarecaps[, colnames(cvrecaps)])
# # } else {
#   cvrecaps = cvarecaps
# # }
```

```{r cva.glmnet call, eval=TRUE, echo=TRUE}
if (!exists("mcvaglmnet")) {
  cvaglmnet = function(idx_train, y_key) {
    x = mget_full_cpg_matrix(idx_train)
    y = mget_df()[idx_train, y_key] # service. Design Oatterns, Gamma et al. 
    modelcva = glmnetUtils::cva.glmnet(x=x, y=y, type.measure="mse", standardize=TRUE)
    return(modelcva)  
  }
  # Use parallel::makeCluster is not compatible with memoisation.
  # cl.cva = parallel::makeCluster(nb_cores,  type="FORK")
  # modelcva = glmnetUtils::cva.glmnet(x=x, y=y, type.measure="mse", standardize=TRUE, outerParallel=cl.cva)
  # parallel::stopCluster(cl.cva)
  mcvaglmnet = memoise::memoise(cvaglmnet)
} # Memoise for cvaglmnet

modelcva = mcvaglmnet(idx_train=idx_train, y_key=y_key)

cvarecaps = data.frame(
  lambda   = unlist(lapply(modelcva$modlist, "[[", "lambda")), 
  cvm      = unlist(lapply(modelcva$modlist, "[[", "cvm")   ), 
  cvup     = unlist(lapply(modelcva$modlist, "[[", "cvup")  ),
  cvlo     = unlist(lapply(modelcva$modlist, "[[", "cvlo")  ),
  nbprobes = unlist(lapply(modelcva$modlist, "[[", "nzero") ),
  alpha    = rep(modelcva$alpha, sapply(lapply(modelcva$modlist, "[[", "lambda"), length))
)
cvarecaps$rmse = sqrt(cvarecaps$cvm)
cvarecaps$rmseup = sqrt(cvarecaps$cvup)
cvarecaps$rmselo = sqrt(cvarecaps$cvlo)
head(cvarecaps)
dim(cvarecaps)

cvrecaps = cvarecaps  
```

```{r plot cvglmnet results, echo=FALSE}
main = paste0("Cross-Validation nb_train=", nb_train)
# Plotting rmse and nbprobes depending on alpha and lambda (with cv results)
layout(matrix(1:2,1),respect = TRUE)
plot(0, 0, col=0, xlab="log10(lambda)", ylab="RMSE", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, min(cvrecaps$rmse)^2))
tmp_alphas = sort(unique(cvrecaps$alpha))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$rmse             , col=adjustcolor(tmp_col, alpha.f=1), lty=2)
  # abline(v=log10(sub_recap$lambda)                      , col=adjustcolor(tmp_col, alpha.f=1), lty=2)
  points(  log10(sub_recap$lambda), sub_recap$rmse  , col=tmp_col, pch=16)
  arrows(x0=log10(sub_recap$lambda), y0=sub_recap$rmselo, x1=log10(sub_recap$lambda), y1=sub_recap$rmseup, code=3, angle=90, length = 0.03, col=tmp_col, lwd=2)
})
plot(0, 0, col=0, xlab="log10(lambda)", ylab="nbprobes", main=main, xlim=log10(range(cvrecaps$lambda)), ylim=c(0, ceiling(nb_train)))
foo = lapply(tmp_alphas, function(alpha) { 
  cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
  sub_recap = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]
  tmp_col = which(alpha==tmp_alphas)
  lines(log10(cvrecap$lambda), cvrecap$nbprobes         , col=adjustcolor(tmp_col, alpha.f=.5), lty=2)
  # abline(v=log10(sub_recap$lambda)                      , col=adjustcolor(tmp_col, alpha.f=.5), lty=2)
  points(  log10(sub_recap$lambda), sub_recap$nbprobes  , col=tmp_col, pch=16)
})
legend(x="topright", legend=tmp_alphas, fill=1:length(tmp_alphas), title="alpha", cex=.5)
```


```{r best model}
best_model = cvrecaps[cvrecaps$rmse==min(cvrecaps$rmse),][1,]
best_model

alpha = best_model[["alpha"]]
lambda = best_model[["lambda"]]
m_params = c(alpha=alpha, lambda=lambda)
model_params = data.frame(alpha=alpha, lambda=lambda)
rownames(model_params) = paste0("glmnet_a", signif(alpha,3), "_l", signif(lambda,3))
```

```{r loading train/test sets}
#Create Train and Test samples
Xtrain = mget_full_cpg_matrix(idx_train)
Ytrain = mget_df()[idx_train,y_key]
Xtest = mget_full_cpg_matrix(idx_test)
Ytest = mget_df()[idx_test,y_key]
``` 


```{r glmnet::glmnet memoisation, echo=FALSE}
# model_factory_glmnet, it produces custom model object based on list by calling glmnet::glmnet function
if (!exists("mmodel_factory_glmnet")) {
  model_factory_glmnet = function(idx_train, y_key, alpha=alpha, lambda=lambda) {
    x = mget_full_cpg_matrix(idx_train)
    y = mget_df()[idx_train,y_key]
    m = glmnet::glmnet(x=x, y=y, alpha=alpha, lambda=lambda, standardize=TRUE) 
    idx = rownames(m$beta)[m$beta@i+1]
    coeff=data.frame(probes=idx, beta=m$beta[idx,])
    rownames(coeff) = idx
    coeff$mean = apply(x[,rownames(coeff)], 2, mean)
    ret = list(Intercept=m$a0, coeff=coeff)
    ret$name = paste0("glmnet_", signif(alpha,3), "_", signif(lambda,3))
    # ret$glmmod = m
    return(ret)
  }
  mmodel_factory_glmnet = memoise::memoise(model_factory_glmnet, cache = cachem::cache_mem(max_size = 10*1024 * 1024^2))
}
```

```{r prediction for best model}
# build model
alpha = m_params[["alpha"]]
lambda = m_params[["lambda"]]
m = mmodel_factory_glmnet(idx_train=idx_train, y_key=y_key, alpha=alpha, lambda=lambda) 
# prediction 
predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
rmseTr = sqrt(mean((Ytrain - predTr)^2))
predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
rmseTe = sqrt(mean((Ytest - predTe)^2))
```

```{r plot prediction for best model, echo=FALSE}
layout(matrix(1:2,1), respect=TRUE)
plot(Ytrain, predTr, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
plot(Ytest , predTe, xlab="Chronological Age", ylab="Predicted Age", main=paste0(m$name, " test  rmse: ", signif(rmseTe, 3))) 
```

```{r glmnet_build_deprecated, echo=FALSE, eval=FALSE}
# ## Model evaluation with best meta parameters
# # best model_params = list(alphas, lambdas)
# model_params = data.frame(t(sapply(alphas, function(alpha, cvrecaps=cvrecaps) {
#   cvrecap = cvrecaps[cvrecaps$alpha==alpha,]
#   # lambda = cvrecap[abs(350-cvrecap$nbprobes)==min(abs(350-cvrecap$nbprobes)),]$lambda[1]
#   lambda = cvrecap[cvrecap$rmse==min(cvrecap$rmse),]$lambda[1]
#   ret = c(alpha=alpha, lambda=lambda)
#   return(ret)
# }, cvrecaps=cvrecaps)))
# # model_params
#
#
# ## Create models
#
# models = apply(model_params, 1, function(m_params, idx_train=idx_train, y_key=y_key) {
#   # print(m_params)
#   alpha = m_params[["alpha"]]
#   lambda = m_params[["lambda"]]
#   m = mmodel_factory_glmnet(idx_train=idx_train, y_key=y_key, alpha=alpha, lambda=lambda)
#   return(m)
# }, idx_train=idx_train, y_key=y_key)
# names(models) = sapply(models, "[[", "name")
#
#
#
# ## Plot predictions with train an test set of each models
#
# #Create Train and Test samples
# Xtrain = mget_full_cpg_matrix(idx_train)
# Ytrain = mget_df()[idx_train,y_key]
# Xtest = mget_full_cpg_matrix(idx_test)
# Ytest = mget_df()[idx_test,y_key]
#
# layout(matrix(1:2,1), respect=TRUE)
# # layout(matrix(1:(length(models)*2),2), respect=TRUE)
# foo = lapply(models, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {
#   predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
#   rmseTr = sqrt(mean((Ytrain - predTr)^2))
#   predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
#   rmseTe = sqrt(mean((Ytest - predTe)^2))
#   plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
#   plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
# }, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```






















# Bootstrap

Bootstrap bootstrap is...

Boostrap increases robustness of results.

#datascientist

```{r bootstrap}
bs_func = function(i, idx_train, model_params) { #epimedtools to see evolution of running 
  # Bootstrap sample creation
  # print(i)
  set.seed(i)
  idx_bstrain = c(
    sample(idx_train, ceiling(2/3 * length(idx_train)), replace=FALSE),
    sample(idx_train, floor(  1/3 * length(idx_train)), replace=TRUE)
  )
  # Create model with best meta params and bootstrap sample
  models_bs = apply(model_params, 1, function(m_params, idx_bstrain, y_key) { 
    # print(m_params)
    alpha = m_params[["alpha"]]
    lambda = m_params[["lambda"]]
    m = mmodel_factory_glmnet(idx_bstrain, y_key, alpha=alpha, lambda=lambda) 
    return(m)  
  }, idx_bstrain=idx_bstrain, y_key=y_key)
  names(models_bs) = paste0(rownames(model_params), "_rnd", i)
  # For each bootstrap, capture all probes use in the model 
  probes = lapply(models_bs, function(m) { 
    probes = rownames(m$coeff)
    return(probes)
  })
  names(probes) = names(models_bs)
  bs = list(probes=probes)
  return(bs)
}

USE_PARAPPLY = FALSE

if (USE_PARAPPLY) {
  print("bootstrap using parallel::parApply...")
  if (!exists("cl_bs")) {
    cl_bs = parallel::makeCluster(parallel::detectCores(),  type="PSOCK")    
    # parallel::stopCluster(cl_bs)
  }
  bs = parallel::parApply(cl_bs, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, model_params=model_params, mmodel_factory_glmnet=mmodel_factory_glmnet)
} else {
  print("bootstrap using epimedtools::monitored_apply")
  bs = epimedtools::monitored_apply(mod=1, t(t(1:n_boot)), 1, bs_func, idx_train=idx_train, model_params=model_params)   
}

bs_probes = unlist(unlist(bs, recursive=FALSE), recursive=FALSE)
length(bs_probes)

# bs_probes = unlist(lapply(bs, "[[", "probes"), recursive=FALSE)
# bs_probes = lapply(bs, "[[", "probes")
# bs_models = bs$models
```


```{r bootstrap distribution}
layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:4,1),respect = TRUE)
boot_probes = lapply(1:nrow(model_params),function(i){
  #  probes = lapply(bs_probes, "[[", i) #Probes for each alpha
  probes = unlist(bs_probes) # probes of all bootstraps
  tmp_tab = table(probes) # distribution of all probes
  tmp_boot_probes = names(tmp_tab)[tmp_tab >= n_boot/2] # probes which appears in more than 50% of models
  barplot(table(table(probes)), las=2, 
    xlab="occurence", 
    main=paste0("Probes occurence distribution for ", n_boot, " bootstrap models based on ", names(model_params)[i]) 
  ) # barplot of occurence of each probes across bootstraps

  barplot(cumsum(rev(table(table(probes)))), las=2, 
    xlab="cumulated occurence", 
    main=paste0("Cumulated probes occurence distribution") 
  ) # barplot of occurence of each probes across bootstraps
  abline(h=length(tmp_boot_probes), lty=2, col=2)
  text(n_boot/4, length(tmp_boot_probes), pos=3, paste0(length(tmp_boot_probes), " probes"), col=2)
  return(tmp_boot_probes)
})
names(boot_probes) = paste0("bs_", rownames(model_params))
```

```{r cv_bs}
#  1. folds
flds = list()
n_fold = 5
fold_size = floor(length(idx_train) / n_fold)
idx_train_remaining = idx_train
set.seed(1)
for (i in 1:n_fold) {
  foo = sample(idx_train_remaining, fold_size)
  flds[[i]] = list(idx_train=setdiff(idx_train, foo), idx_test=foo)
  idx_train_remaining = setdiff(idx_train, unlist(lapply(flds, "[[", "idx_test")))
}

# 2. eval
RMSE = function(data_truth, data_pred) {
    # Root Mean Square Error
    return(sqrt(mean((data_truth - data_pred)^2)))
}

probes = unlist(bs_probes) # probes of all bootstraps
tmp_tab = table(probes) # distribution of all probes
tmp_idx_probes = unique(names(tmp_tab))
sub_df = df[,c(tmp_idx_probes,y_key)] 

stats = lapply(unique(tmp_tab[tmp_tab>=n_boot/2]), function(occ) {
  # print(occ)
  tmp_probes = names(tmp_tab)[tmp_tab>=occ]

  stats = lapply(1:n_fold, function(fold) {
    # print(fold)
    tmp_idx_train = flds[[fold]]$idx_train
    tmp_idx_test = flds[[fold]]$idx_test

    m = lm(formula(paste0(y_key,"~0+",paste0(tmp_probes, collapse="+"))), data=sub_df[tmp_idx_train,])
    train_pred = predict(m, sub_df[tmp_idx_train,], type="response")
    train_truth = sub_df[tmp_idx_train, y_key]
    train_err = RMSE(train_truth, train_pred)
    test_pred = predict(m, sub_df[tmp_idx_test,], type="response")
    test_truth = sub_df[tmp_idx_test, y_key]
    test_err = RMSE(test_truth, test_pred)
    # print(paste0("nb_probes:", length(tmp_probes), ", train_err: ", signif(train_err, 3), ", test_err: ", signif(test_err, 3)))
    ret = data.frame(
      occurence=occ, 
      nb_probes=length(tmp_probes), 
      train_err=train_err, 
      test_err=test_err,
      fold=fold
    )
    return(ret)
  })
  stats = do.call(rbind, stats)
  stats
})
stats = do.call(rbind, stats)
head(stats)

# 3. plot
layout(matrix(1:2,1),respect = TRUE)
plot  (stats$occurence, stats$train_err, col=1, xlab="probes occurence", ylab="RMSE", main="Cross Validation")
points(stats$occurence, stats$test_err, col=2)
plot  (stats$nb_probes, stats$train_err, col=1, xlab="nb_probes", ylab="RMSE", main="Cross Validation")
points(stats$nb_probes, stats$test_err, col=2)
legend("bottomleft", col=1:2, c("train", "test"), pch=1)
```


```{r}
probes = unlist(bs_probes) # probes of all bootstraps
tmp_tab = rev(sort(table(probes))) # distribution of all probes

tmp_idx_probes = unique(names(tmp_tab))
tmp_train = df[idx_train,c(tmp_idx_probes,y_key)] # train set with CpG and y_key for lm

for (i in unique(tmp_tab[tmp_tab>=n_boot/2])) {
  print(i)
  tmp_probes = names(tmp_tab)[tmp_tab>=i]
  m = lm(formula(paste0(y_key,"~",paste0(tmp_probes,collapse="+"))),data=tmp_train) #lm model with probes previously find with bootstrap
  idx = tmp_probes
  coeff = data.frame(probes=idx, beta=m$coefficients[idx])
  rownames(coeff) = idx
  coeff$mean = apply(tmp_train[,rownames(coeff)], 2, mean)
  head(coeff)
  m = list(Intercept=m$coefficients[[1]], coeff=coeff)


  layout(matrix(1:2,1), respect=TRUE)
    predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
    rmseTr = sqrt(mean((Ytrain - predTr)^2))
    predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
    rmseTe = sqrt(mean((Ytest - predTe)^2))
    plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0("occurence: ", i, m$name, " train rmse: ", signif(rmseTr, 3)))
    plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0("occurence: ", i, m$name, " test  rmse: ", signif(rmseTe, 3)))
}
```




















# Results and Discussion

For each alpha, we see the occurence of each probes across bootstraps.


Discussion, a **core of probes** are always selected by bootstrap.

- What is the relevance of the clock built with only this core of probes? #biostat
- Which one? What are associated biologiocal functions? #biologist 
- Are these probes located into wild Differebnttion Metrhylation Region (DMR)? #biologist


## Hannum probes

For each alpha, we see the occurence of Hannum probes across bootstraps.

```{r hannum distribution}
layout(1, respect=TRUE)
# layout(matrix(1:2,1),respect = TRUE)
if (!exists("mget_coefHannum")) mget_coefHannum = memoise::memoise(methylclockData::get_coefHannum)
hannum_coeff = mget_coefHannum() # Get Hannum probes
foo = lapply(1:nrow(model_params), function(i) {
  # probes = lapply(bs_probes, "[[", i)
  probes = unlist(bs_probes)
  tmp_tab = table(probes)
  tmp_tab_Hannum = tmp_tab[hannum_coeff$CpGmarker] # Occurence of each Hannum's probes
  tmp_tab_Hannum[is.na(tmp_tab_Hannum)] = 0  # If no occurences, set 0 
  barplot(table(tmp_tab_Hannum), las=2, main=paste0("Hannum probes in ", rownames(model_params)[i], " ", sum(tmp_tab_Hannum >= n_boot/2), " probes.")) # Plot occurences of Hannum probes across bootstraps
  ret = names(tmp_tab_Hannum)[tmp_tab_Hannum >= n_boot/2]
  return(ret)
})
unique(unlist(foo))
```

## Occurrence of probes along of boostrap process

Follow the number of retained probes along of the bootstraps process

```{r probes_occurrence }
layout(matrix(1:2,1),respect = TRUE)
# layout(matrix(1:(length(models)*2),2), respect=TRUE)
x = seq(2,n_boot,2) # Sequence of even bootstraps
for (i in 1:nrow(model_params)){ #for each alpha
  tmp_probes = lapply(x, function(nb_boot){ # when the number of bootstrap is even 
    tmp_probes = unlist(bs_probes[1:nb_boot]) 
    tmp_tab = table(tmp_probes)
    tmp_boot_cpg = names(tmp_tab)[tmp_tab >= nb_boot/2] # Check probes appears more than 50% times
    return(tmp_boot_cpg)
  })
  # barplot(sort(table(unlist(tmp_probes))))
  nb_probes = unlist(sapply(tmp_probes, length)) # Check number of probes across boostraps
  plot(x, nb_probes, type = "l", main = paste0("alpha: ", rownames(model_params)[i])) # Plot number of probes across bootstraps
  tab_probes = table(nb_probes) 
  barplot(tab_probes, las=2) # Check when number of probes stabilised
}
# tab_probes
```























```{r stock bootstrap model}
tmp_idx_probes = unique(unlist(boot_probes))
tmp_train = df[idx_train,c(tmp_idx_probes,y_key)] # train set with CpG and y_key for lm
models_bs = lapply(1:length(boot_probes), function(i){ # Compute coefficients and mean in the train for each probes of the model for each alpha
	m = lm(formula(paste0(y_key,"~",paste0(boot_probes[[i]],collapse="+"))),data=tmp_train) #lm model with probes previously find with bootstrap
  idx = boot_probes[[i]]
  coeff = data.frame(probes=idx, beta=m$coefficients[idx])
  rownames(coeff) = idx
  coeff$mean = apply(tmp_train[,rownames(coeff)], 2, mean)
  head(coeff)
  ret = list(Intercept=m$coefficients[[1]], coeff=coeff)
  ret$name = names(boot_probes)[i]
  # ret$glmmod = m
  return(ret)
})
names(models_bs) = sapply(models_bs, "[[", "name")
```

```{r rmse model}
#Plot rmse like with cross validation but with bootstrap results
layout(matrix(1:2,1), respect=TRUE)
# layout(matrix(1:8,2), respect=TRUE)
foo = lapply(models_bs, function(m, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest) {               
  predTr = Xtrain[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTr = sqrt(mean((Ytrain - predTr)^2))
  predTe = Xtest[,rownames(m$coeff)] %*% as.matrix(m$coeff$beta) + m$Intercept
  rmseTe = sqrt(mean((Ytest - predTe)^2))
  plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
  plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))  
}, Xtrain=Xtrain, Ytrain=Ytrain, Xtest=Xtest, Ytest=Ytest)
```


# AMAR




```{r amar plotting, eval = FALSE}
layout(matrix(1:3,1), respect=TRUE)

models = list(m,models_bs[[1]])
litterature_models = readRDS("litterature_models.rds")
models = c(models,litterature_models)


for (it in covariates) {
  factors = unique(df[,it])
  tmp_col = RColorBrewer::brewer.pal(n=max(3,length(factors)), name = "Set1")
  for (i in 1:length(models)) {
    m = models[[i]]
  
    tmp_Xtrain = Xtrain[,rownames(m$coeff)[rownames(m$coeff) %in% colnames(Xtrain)]] #Keep only common CpG between coeffs and Xtrain
    tmp_Xtest = Xtest[,rownames(m$coeff)[rownames(m$coeff) %in% colnames(Xtest)]] #Keep only common CpG between coeffs and Xtrain
  
    idx_missing_probes = rownames(m$coeff)[!(rownames(m$coeff) %in% colnames(Xtrain))]
    if (length(idx_missing_probes) != 0) {
      foo = m$coeff[idx_missing_probes,"mean"]
      names(foo) = idx_missing_probes
      tmp_Xtrain = cbind(tmp_Xtrain,do.call("rbind",replicate(nrow(tmp_Xtrain),foo,simplify=FALSE)))
      tmp_Xtest = cbind(tmp_Xtest,do.call("rbind",replicate(nrow(tmp_Xtest),foo,simplify=FALSE)))
    }
  
    tmp_Xtrain = tmp_Xtrain[,rownames(m$coeff)]
    tmp_Xtest = tmp_Xtest[,rownames(m$coeff)]
  
    predTr = tmp_Xtrain %*% as.matrix(m$coeff$beta) + m$Intercept
    rmseTr = sqrt(mean((Ytrain - predTr)^2))
    predTe = tmp_Xtest %*% as.matrix(m$coeff$beta) + m$Intercept
    rmseTe = sqrt(mean((Ytest - predTe)^2))
  
    plot(predTr, Ytrain, xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " train rmse: ", signif(rmseTr, 3)))
    plot(predTe, Ytest , xlab="Predicted Age", ylab="Chronological Age", main = paste0(m$name, " test  rmse: ", signif(rmseTe, 3)))
  
    AMAR_Te = predTe/Ytest
    AMAR_Te = cbind.data.frame(AMAR_Te,df[rownames(AMAR_Te),it])
    colnames(AMAR_Te) = c("AMAR",it)
  
    density = lapply(factors, function(factor){
      if(sum(AMAR_Te[,it] == factor) > 1) {
        return(density(AMAR_Te[AMAR_Te[,it] == factor, "AMAR"]))
      } # Problem density
    })
    plot(0,0, xlim = c(min(unlist(lapply(density, function(d){min(d$x)}))),max(unlist(lapply(density, function(d){max(d$x)})))), ylim = c(min(unlist(lapply(density, function(d){min(d$y)}))),max(unlist(lapply(density, function(d){max(d$y)})))), main = paste0("density for ",m$name))
		for (j in c(1:length(factors))){
			lines(density[[j]],col = tmp_col[j])
		}
		legend(x="topleft", legend=factors, col=tmp_col, lty = 1, title="factor")
  }
}
```






# Session Information

```{r, results="verbatim"}
end_time = Sys.time()
print(paste0("Execution time for vignette : ", end_time - start_time))
sessionInfo()
```